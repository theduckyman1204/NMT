{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":14625,"status":"ok","timestamp":1730907196901,"user":{"displayName":"Đặng Minh Trí","userId":"13831946193514353260"},"user_tz":-420},"id":"m4FPX-IYApCW","outputId":"caddecfa-f965-460f-ab67-7ebfd7fa56a0","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0mTraceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 78, in main\n","    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/__init__.py\", line 114, in create_command\n","    module = importlib.import_module(module_path)\n","  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 15, in <module>\n","    from pip._internal.cli.req_command import (\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 19, in <module>\n","    from pip._internal.index.package_finder import PackageFinder\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/index/package_finder.py\", line 31, in <module>\n","    from pip._internal.req import InstallRequirement\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/__init__.py\", line 9, in <module>\n","    from .req_install import InstallRequirement\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 40, in <module>\n","    from pip._internal.operations.install.wheel import install_wheel\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/install/wheel.py\", line 40, in <module>\n","    from pip._vendor.distlib.scripts import ScriptMaker\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/distlib/scripts.py\", line 17, in <module>\n","    from .resources import finder\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/distlib/resources.py\", line 19, in <module>\n","    from .util import cached_property, get_cache_base, Cache\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/distlib/util.py\", line 23, in <module>\n","    import tarfile\n","  File \"/usr/lib/python3.10/tarfile.py\", line 1637, in <module>\n","    class TarFile(object):\n","KeyboardInterrupt\n","^C\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["! pip -q install streamlit\n","! pip -q install torchtext==0.6.0\n","! pip -q install dill"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1730907196901,"user":{"displayName":"Đặng Minh Trí","userId":"13831946193514353260"},"user_tz":-420},"id":"Mey30HYHBgsU","outputId":"e3727a8b-a44b-4fc0-a927-3bd8a55edc9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","\n","import streamlit as st\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import math\n","import dill\n","from torchtext import data\n","from nltk.corpus import wordnet\n","import re\n","import pandas as pd\n","import spacy\n","import re\n","\n","\n","with open(\"/content/drive/MyDrive/Tri/NLP/Save Model/BTL_1/model.pkl\", \"rb\") as f:\n","    model = dill.load(f)\n","\n","def attention(q, k, v, mask=None, dropout=None):\n","    \"\"\"\n","    q: batch_size x head x seq_length x d_model\n","    k: batch_size x head x seq_length x d_model\n","    v: batch_size x head x seq_length x d_model\n","    mask: batch_size x 1 x 1 x seq_length\n","    output: batch_size x head x seq_length x d_model\n","    \"\"\"\n","\n","    # attention score được tính bằng cách nhân q với k\n","    d_k = q.size(-1)\n","    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n","\n","    if mask is not None:\n","        mask = mask.unsqueeze(1)\n","        scores = scores.masked_fill(mask==0, -1e9)\n","    # xong rồi thì chuẩn hóa bằng softmax\n","    scores = F.softmax(scores, dim=-1)\n","\n","    if dropout is not None:\n","        scores = dropout(scores)\n","\n","    output = torch.matmul(scores, v)\n","    return output, scores\n","\n","\n","class MyIterator(data.Iterator):\n","    def create_batches(self):\n","        if self.train:\n","            def pool(d, random_shuffler):\n","                for p in data.batch(d, self.batch_size * 100):\n","                    p_batch = data.batch(\n","                        sorted(p, key=self.sort_key),\n","                        self.batch_size, self.batch_size_fn)\n","                    for b in random_shuffler(list(p_batch)):\n","                        yield b\n","            self.batches = pool(self.data(), self.random_shuffler)\n","\n","        else:\n","            self.batches = []\n","            for b in data.batch(self.data(), self.batch_size,self.batch_size_fn):\n","                self.batches.append(sorted(b, key=self.sort_key))\n","\n","global max_src_in_batch, max_tgt_in_batch\n","\n","@st.cache_data\n","def batch_size_fn(new, count, sofar):\n","    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n","    global max_src_in_batch, max_tgt_in_batch\n","    if count == 1:\n","        max_src_in_batch = 0\n","        max_tgt_in_batch = 0\n","    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n","    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n","    src_elements = count * max_src_in_batch\n","    tgt_elements = count * max_tgt_in_batch\n","    return max(src_elements, tgt_elements)\n","\n","@st.cache_data\n","def nopeak_mask(size, device):\n","    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n","    mô hình không nhìn thấy được các từ ở tương lai\n","    \"\"\"\n","    np_mask = np.triu(np.ones((1, size, size)),\n","    k=1).astype('uint8')\n","    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n","    np_mask = np_mask.to(device)\n","\n","    return np_mask\n","@st.cache_data\n","def create_masks(src, trg, src_pad, trg_pad, device):\n","    \"\"\" Tạo mask cho encoder,\n","    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào\n","    \"\"\"\n","    src_mask = (src != src_pad).unsqueeze(-2)\n","\n","    if trg is not None:\n","        trg_mask = (trg != trg_pad).unsqueeze(-2)\n","        size = trg.size(1) # get seq_len for matrix\n","        np_mask = nopeak_mask(size, device)\n","        if trg.is_cuda:\n","            np_mask.cuda()\n","        trg_mask = trg_mask & np_mask\n","\n","    else:\n","        trg_mask = None\n","    return src_mask, trg_mask\n","\n","\n","def get_synonym(word, SRC):\n","    syns = wordnet.synsets(word)\n","    for s in syns:\n","        for l in s.lemmas():\n","            if SRC.vocab.stoi[l.name()] != 0:\n","                return SRC.vocab.stoi[l.name()]\n","    return 0\n","\n","@st.cache_data\n","def multiple_replace(dict, text):\n","    # Create a regular expression  from the dictionary keys\n","    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n","\n","    # For each match, look-up corresponding value in dictionary\n","    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n","\n","\n","def init_vars(src, model, SRC, TRG, device, k, max_len):\n","    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n","    \"\"\"\n","    init_tok = TRG.vocab.stoi['<sos>']\n","    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n","    # tính sẵn output của encoder\n","    e_output = model.encoder(src, src_mask)\n","    outputs = torch.LongTensor([[init_tok]])\n","\n","    outputs = outputs.to(device)\n","\n","    trg_mask = nopeak_mask(1, device)\n","    # dự đoán kí tự đầu tiên\n","    out = model.out(model.decoder(outputs,\n","    e_output, src_mask, trg_mask))\n","    out = F.softmax(out, dim=-1)\n","\n","    probs, ix = out[:, -1].data.topk(k)\n","    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n","\n","    outputs = torch.zeros(k, max_len).long()\n","    outputs = outputs.to(device)\n","    outputs[:, 0] = init_tok\n","    outputs[:, 1] = ix[0]\n","\n","    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n","\n","    e_outputs = e_outputs.to(device)\n","    e_outputs[:, :] = e_output[0]\n","\n","    return outputs, e_outputs, log_scores\n","\n","def k_best_outputs(outputs, out, log_scores, i, k):\n","\n","    probs, ix = out[:, -1].data.topk(k)\n","    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n","    k_probs, k_ix = log_probs.view(-1).topk(k)\n","\n","    row = k_ix // k\n","    col = k_ix % k\n","\n","    outputs[:, :i] = outputs[row, :i]\n","    outputs[:, i] = ix[row, col]\n","\n","    log_scores = k_probs.unsqueeze(0)\n","\n","    return outputs, log_scores\n","\n","def beam_search(src, model, SRC, TRG, device, k, max_len):\n","\n","    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n","    eos_tok = TRG.vocab.stoi['<eos>']\n","    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n","    ind = None\n","    for i in range(2, max_len):\n","\n","        trg_mask = nopeak_mask(i, device)\n","\n","        out = model.out(model.decoder(outputs[:,:i],\n","        e_outputs, src_mask, trg_mask))\n","\n","        out = F.softmax(out, dim=-1)\n","\n","        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n","\n","        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n","        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n","        for vec in ones:\n","            i = vec[0]\n","            if sentence_lengths[i]==0: # First end symbol has not been found yet\n","                sentence_lengths[i] = vec[1] # Position of first end symbol\n","\n","        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n","\n","        if num_finished_sentences == k:\n","            alpha = 0.7\n","            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n","            _, ind = torch.max(log_scores * div, 1)\n","            ind = ind.data[0]\n","            break\n","\n","    if ind is None:\n","\n","        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n","        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n","\n","    else:\n","        length = (outputs[ind]==eos_tok).nonzero()[0]\n","        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n","\n","\n","def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n","    \"\"\"Dịch một câu sử dụng beamsearch\n","    \"\"\"\n","    model.eval()\n","    indexed = []\n","    sentence = SRC.preprocess(sentence)\n","\n","    for tok in sentence:\n","        if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n","            indexed.append(SRC.vocab.stoi[tok])\n","        else:\n","            indexed.append(get_synonym(tok, SRC))\n","\n","    sentence = Variable(torch.LongTensor([indexed]))\n","    sentence = sentence.to(device)\n","    sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n","    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)\n","\n","@st.cache_resource\n","class tokenize(object):\n","\n","    def __init__(self, lang):\n","        self.nlp = spacy.load(lang)\n","\n","    def tokenizer(self, sentence):\n","        sentence = re.sub(\n","        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n","        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n","        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n","        sentence = re.sub(r\"\\,+\", \",\", sentence)\n","        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n","        sentence = sentence.lower()\n","        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]\n","\n","@st.cache_data\n","def read_data(src_file, trg_file):\n","    src_data = open(src_file).read().strip().split('\\n')\n","\n","    trg_data = open(trg_file).read().strip().split('\\n')\n","\n","    return src_data, trg_data\n","@st.cache_resource\n","def create_fields(src_lang, trg_lang):\n","\n","    print(\"loading spacy tokenizers...\")\n","\n","    t_src = tokenize(src_lang)\n","    t_trg = tokenize(trg_lang)\n","\n","    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n","    SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n","\n","    return SRC, TRG\n","\n","def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n","\n","    print(\"creating dataset and iterator... \")\n","\n","    raw_data = {'src' : [line for line in src_data], 'trg': [line for line in trg_data]}\n","    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n","\n","    mask = (df['src'].str.count(' ') < max_strlen) & (df['trg'].str.count(' ') < max_strlen)\n","    df = df.loc[mask]\n","\n","    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n","\n","    data_fields = [('src', SRC), ('trg', TRG)]\n","    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n","\n","    train_iter = MyIterator(train, batch_size=batchsize, device=device,\n","                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n","                        batch_size_fn=batch_size_fn, train=istrain, shuffle=True)\n","\n","    os.remove('translate_transformer_temp.csv')\n","\n","    if istrain:\n","        SRC.build_vocab(train)\n","        TRG.build_vocab(train)\n","\n","    return train_iter\n","\n","\n","opt = {\n","    'train_src_data':'/content/drive/MyDrive/Tri/NLP/Data/Train/train.en',\n","    'train_trg_data':'/content/drive/MyDrive/Tri/NLP/Data/Train/train.vi',\n","    'valid_src_data':'/content/drive/MyDrive/Tri/NLP/Data/Test/tst2013.en',\n","    'valid_trg_data':'/content/drive/MyDrive/Tri/NLP/Data/Test/tst2013.vi',\n","    'src_lang':'en_core_web_sm',\n","    'trg_lang':'en_core_web_sm',\n","    'max_strlen':160,\n","    'batchsize':1000,\n","    'device':'cuda',\n","    'd_model': 512,\n","    'n_layers': 6,\n","    'heads': 8,\n","    'dropout': 0.1,\n","    'lr':0.0001,\n","    'epochs':30,\n","    'printevery': 200,\n","    'k':5,\n","}\n","@st.cache_resource\n","def init(opt):\n","    train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n","    valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n","    SRC, TRG = create_fields(opt['src_lang'], opt['trg_lang'])\n","    train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n","    valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=False)\n","    src_pad = SRC.vocab.stoi['<pad>']\n","    trg_pad = TRG.vocab.stoi['<pad>']\n","    return SRC, TRG\n","\n","SRC, TRG = init(opt=opt)\n","\n","st.title('Neural Machine Translation')\n","\n","# Initialize chat history\n","if \"messages\" not in st.session_state:\n","    st.session_state.messages = []\n","\n","# Display chat messages from history on app rerun\n","for message in st.session_state.messages:\n","    with st.chat_message(message[\"role\"]):\n","        st.markdown(message[\"content\"])\n","trans_sent = 'Welcome to NMT, please tell me the sentence you want to translate!'\n","# React to user input\n","if prompt := st.chat_input(\"Text need translation\"):\n","    # Display user message in chat message container\n","    with st.chat_message(\"user\"):\n","        st.markdown(prompt)\n","    # Add user message to chat history\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    sentence= prompt\n","    trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n","\n","response = f\"NMT: {trans_sent}\"\n","# Display assistant response in chat message container\n","with st.chat_message(\"assistant\"):\n","    st.markdown(response)\n","# Add assistant response to chat history\n","st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1730907196901,"user":{"displayName":"Đặng Minh Trí","userId":"13831946193514353260"},"user_tz":-420},"id":"KrSY9csCCw2z"},"outputs":[],"source":["!streamlit run /content/app.py &>/content/logs.txt &"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WE62ej4UFnpG","executionInfo":{"status":"ok","timestamp":1730907205467,"user_tz":-420,"elapsed":8569,"user":{"displayName":"Đặng Minh Trí","userId":"13831946193514353260"}},"outputId":"816aa627-c004-4b0f-8752-0d7347698e45"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1G\u001b[0JNeed to install the following packages:\r\n","  localtunnel@2.0.2\r\n","Ok to proceed? (y) \u001b[20G^C\n"]}],"source":["!npx localtunnel --port 8501"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1LhjbiwqkQFwL-EHc0IzOhyGSY8mOpaQX","authorship_tag":"ABX9TyOnedKfCQCjoxWJYeqTC6Ib"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}